```python
from langchain.chat_models import ChatOpenAI

# chat_model은 대화에 더욱 최적화 = LLM이 질문을 받기만 하는게 아니라 LLM과 대화를 할 수 있다.
chat = ChatOpenAI(temperature=0.1)

# 하나의 텍스트(String)를 predict 하는게 아니라 --> chat.predict(...)
# message들을 predict 해보자
from langchain.schema import HumanMessage, AIMessage, SystemMessage # message Constructor

# SystemMessage --> AI(LLM)에 기본 설정, 기본 값, 기본 context를 제공하기 위한 메시지
# AIMessage     --> AI에 의해 보내지는 메시지
# HumanMessage  --> 사람이 보내는 메시지

messages = [
    # LLM 기본 설정
    SystemMessage(content="You are a geography expert. And you only reply in Korean."),
    # 일종의 가상 대화, '대화의 맥락에서 AI가 이렇게 말을 한 것이다.'라고 생각하면 된다.
    # 참고로 '메모리(memory)'에 이러한 정보가 저장된다.
    AIMessage(content="안녕하세요, 내 이름은 테리우스입니다."),
    # 위의 AIMessage(가상 대화) 다음에 사용자로서 사람이 던지는 질문
    HumanMessage(content="""What is the distance between Mexico and Thailand. Also, what is your name?""")
]

chat.predict_messages(messages)
```